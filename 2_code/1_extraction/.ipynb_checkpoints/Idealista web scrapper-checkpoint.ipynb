{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers -> Ctrl + shift + i (pagina de desarrollador) -> network Ctrl + f5 -> primera pestaña -> request headers\n",
    "# desde \"accept\" ponerlo en dict \n",
    "headers = {\n",
    "\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\"accept-encoding\": \"gzip, deflate, br\",\n",
    "\"accept-language\": \"en-GB,en;q=0.9,en-US;q=0.8,ca;q=0.7,es;q=0.6,eo;q=0.5\",\n",
    "\"cache-control\": \"no-cache\",\n",
    "\"cookie\": '''atuserid=%7B%22name%22%3A%22atuserid%22%2C%22val%22%3A%2268f673fe-430e-424d-9df0-0cd8699a34a7%22%2C%22options%22%3A%7B%22end%22%3A%222023-03-03T18%3A46%3A49.424Z%22%2C%22path%22%3A%22%2F%22%7D%7D; atidvisitor=%7B%22name%22%3A%22atidvisitor%22%2C%22val%22%3A%7B%22vrn%22%3A%22-582065-%22%7D%2C%22options%22%3A%7B%22path%22%3A%22%2F%22%2C%22session%22%3A15724800%2C%22end%22%3A15724800%7D%7D; didomi_token=eyJ1c2VyX2lkIjoiMTdlYWM0ZmEtNzcxNC02MGMyLWJmODMtYWRjMGViMzFiYzE3IiwiY3JlYXRlZCI6IjIwMjItMDEtMzBUMTg6NDY6NTEuMTA0WiIsInVwZGF0ZWQiOiIyMDIyLTAxLTMwVDE4OjQ2OjUxLjEwNFoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsiZ29vZ2xlIiwiYzptaXhwYW5lbCIsImM6YWJ0YXN0eS1MTGtFQ0NqOCIsImM6aG90amFyIiwiYzp5YW5kZXhtZXRyaWNzIiwiYzpiZWFtZXItSDd0cjdIaXgiLCJjOmFwcHNmbHllci1HVVZQTHBZWSIsImM6dGVhbGl1bWNvLURWRENkOFpQIiwiYzppZGVhbGlzdGEtTHp0QmVxRTMiLCJjOmlkZWFsaXN0YS1mZVJFamUyYyJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJhbmFseXRpY3MtSHBCSnJySzciLCJnZW9sb2NhdGlvbl9kYXRhIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkFGbUFDQUZrLkFBQUEifQ==; euconsent-v2=CPTmlIAPTmlIAAHABBENB_CoAP_AAAAAAAAAF5wBAAIAAtAC2AvMAAABAaADAAEEQyUAGAAIIhlIAMAAQRDIQAYAAgiGOgAwABBEMJABgACCIYyADAAEEQxUAGAAIIhg.f_gAAAAAAAAA; _gcl_au=1.1.8146483.1643568415; _fbp=fb.1.1643568415299.2070506742; afUserId=58c2a54c-4375-4eb4-baca-6a6e48c65725-p; AF_SYNC=1643568416416; askToSaveAlertPopUp=true; _hjSessionUser_250321=eyJpZCI6ImU0N2FkYjkyLTgyMDMtNWU0Mi1iMTRiLWIzMzQzMDE0NTM0NiIsImNyZWF0ZWQiOjE2NDM1Njg1MjgzMzYsImV4aXN0aW5nIjp0cnVlfQ==; userUUID=6eadbb28-4981-4c13-abda-fd7fadaec832; SESSION=f1f80941eb4fade2~bda569d4-9873-4a74-80a2-5fd0ae1a6c25; contactbda569d4-9873-4a74-80a2-5fd0ae1a6c25=\"{'email':null,'phone':null,'phonePrefix':null,'friendEmails':null,'name':null,'message':null,'message2Friends':null,'maxNumberContactsAllow':10,'defaultMessage':true}\"; cookieSearch-1=\"/alquiler-viviendas/barcelona/les-corts/les-corts/:1643650811421\"; sendbda569d4-9873-4a74-80a2-5fd0ae1a6c25=\"{'friendsEmail':null,'email':null,'message':null}\"; _hjIncludedInSessionSample=1; _hjSession_250321=eyJpZCI6IjYyNTE2ZTJjLWNkNTYtNDRkZS05NmJkLWQ4ZjYxYjQwMGE1NCIsImNyZWF0ZWQiOjE2NDM2NTEzNjU5MTEsImluU2FtcGxlIjp0cnVlfQ==; _hjAbsoluteSessionInProgress=0; _hjCachedUserAttributes=eyJhdHRyaWJ1dGVzIjp7ImlkX3BhZ2VMYW5ndWFnZSI6ImVzIiwiaWRfdXNlclJvbGUiOiIifSwidXNlcklkIjpudWxsfQ==; ABTasty=uid=s43fcyc0hr2z8nkz&fst=1643568412317&pst=1643568412317&cst=1643650069991&ns=2&pvt=35&pvis=13&th=; ABTastySession=mrasn=&sen=12&lp=https%253A%252F%252Fwww.idealista.com%252F; utag_main=v_id:017eac27920f001f94d41f93932f05072002f06a00978$_sn:3$_se:17$_ss:0$_st:1643653573939$dc_visit:2$ses_id:1643650066499%3Bexp-session$_pn:13%3Bexp-session$_prevVtSource:directTraffic%3Bexp-1643653668218$_prevVtCampaignCode:%3Bexp-1643653668218$_prevVtDomainReferrer:%3Bexp-1643653668218$_prevVtSubdomaninReferrer:%3Bexp-1643653668218$_prevVtUrlReferrer:%3Bexp-1643653668218$_prevVtCampaignLinkName:%3Bexp-1643653668218$_prevVtCampaignName:%3Bexp-1643653668218$_prevVtRecommendationId:%3Bexp-1643653668218$_prevCompletePageName:12%3A%3Adetail%3A%3A%3A%3A%3A%3Ahome%3Bexp-1643655375478$_prevLevel2:12%3Bexp-1643655375478$_prevAdId:94476328%3Bexp-1643655375489$_prevAdOriginTypeRecommended:undefined%3Bexp-1643653668228$dc_event:5%3Bexp-session$dc_region:us-east-1%3Bexp-session; cto_bundle=5izo9l9XUkgzNEJIYU9ITGFNWU5xMWp4YWoxNCUyRnFJN2dHbFJTbWRCVlpldHRFbjF4aFFaSmFvWHZvV3B2WCUyRmx1MEhiMUNQbDlUQmlPckkwdW51VTdTZjltc255c0ZjRng0d1c4MXVFVURBdUFXdEYwaSUyQnhtSlowY3oybzE4TkxtY2RVWmR3ZHJvbVJvUko1MmQxT1FJczJvV2clM0QlM0Q; datadome=v475FOZILU7oISIcQM..EhoU-BUZiH-ZFK2rcgXnvpb4HM05afYuAyFXtJCwx2Wz7bUmxBeuhegZikjdshl3ZtuSvzBOApWI_EnGCatwDUdNRGdY_Ox0oK6H5CgXTyT''',\n",
    "\"pragma\": \"no-cache\",\n",
    "\"referer\": \"https://www.idealista.com/inmueble/95249555/\",\n",
    "\"sec-ch-ua\": '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"97\", \"Chromium\";v=\"97\"',\n",
    "\"sec-ch-ua-mobile\": \"?0\",\n",
    "\"sec-ch-ua-platform\": '\"Windows\"',\n",
    "\"sec-fetch-dest\": \"document\",\n",
    "\"sec-fetch-mode\": \"navigate\",\n",
    "\"sec-fetch-site\": \"same-origin\",\n",
    "\"sec-fetch-user\": \"?1\",\n",
    "\"upgrade-insecure-requests\": \"1\",\n",
    "\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\"\n",
    "    \n",
    "}\n",
    "\n",
    "# headers = {'User-agent': 'your bot 0.1'} # no funciona necesita datos reales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion usar ids y devolver caracteristicas inmueble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(headers, cp):\n",
    "    '''\n",
    "    Function that returns all the flats ids of certain zip code with fixed headers request.\n",
    "    The function return a list with the ids.\n",
    "    \n",
    "    headers: needed to obtain response 200 (success).\n",
    "    cp: zip code.\n",
    "    \n",
    "    Returns list of ids.\n",
    "    '''\n",
    "    ids = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        # Get the first page of results\n",
    "        url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/'\n",
    "        r = session.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        soup = bs(r.text, 'lxml')\n",
    "        articles = soup.find_all('article', {'class': 'item'})\n",
    "        ids += [article.get('data-adid') for article in articles]\n",
    "\n",
    "        x=2\n",
    "        \n",
    "    while True:\n",
    "        url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/pagina-{x}.htm'\n",
    "        r = session.get(url, headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            break\n",
    "        soup = bs(r.text, 'lxml')\n",
    "\n",
    "        if soup.find('main', {'class':'listing-items'}).find('div',{'class':'pagination'}) == None:\n",
    "            break\n",
    "\n",
    "\n",
    "        pag_actual = int(soup.find('main', {'class':'listing-items'}).find('div',{'class':'pagination'}).find('li',{'class':'selected'}).text)\n",
    "\n",
    "#         print(x,pag_actual)\n",
    "        if x == pag_actual:\n",
    "                    articles = soup.find('main', {'class':'listing-items'}).find_all('article')\n",
    "        else:\n",
    "            break\n",
    "        articles = soup.find_all('article', {'class': 'item'})\n",
    "        if not articles:\n",
    "            break\n",
    "        ids += [article.get('data-adid') for article in articles]\n",
    "        x += 1\n",
    "        time.sleep(random.randint(3, 4) * random.random())\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data_from_ids(ids, headers, cp):\n",
    "    '''\n",
    "    Function that return all the data from each add id from previous function \"get_ids\".\n",
    "    It returns the data in dictionary format, ready to be converted to dataframe.\n",
    "\n",
    "    ids = list of ids.\n",
    "    headers = to avoid errors, response 200 (success)\n",
    "    cp = zip code of the ids\n",
    "\n",
    "    Returns a dic with id as a key.\n",
    "    '''\n",
    "\n",
    "    dic = {}\n",
    "    url = \"https://www.idealista.com/inmueble/{}/\"\n",
    "    \n",
    "    regex_m2 = re.compile(r'\\d\\d\\d m²|\\d\\d m²')\n",
    "    regex_hab = re.compile(r'\\d hab|sin habitación')\n",
    "    regex_baño = re.compile(r'\\d baño')\n",
    "    regex_planta = re.compile(r'planta \\d\\d|planta \\d|bajo')\n",
    "    \n",
    "    x=1\n",
    "    \n",
    "    for i in ids:\n",
    "        print(\"aqui entra en el bucle\")\n",
    "        with requests.Session() as session:\n",
    "            try:\n",
    "                print(\"aqui esta en el try\")\n",
    "                r = session.get(url.format(i), headers=headers)\n",
    "                r.raise_for_status()\n",
    "\n",
    "                soup = bs(r.text, 'lxml')  # formato mas legible\n",
    "\n",
    "                # ubicacion\n",
    "                ubi = soup.select(\"#headerMap li\")\n",
    "                ubicacion_full = '|'.join([u.get_text(strip=True) for u in ubi])  # string\n",
    "\n",
    "                titulo = soup.select_one('h1').get_text(strip=True)\n",
    "                localizacion = soup.select_one('.main-info__title-block span').get_text(strip=True)\n",
    "\n",
    "                # ubi table\n",
    "                ubicacion_items = [re.sub(r'\\n+', '', u.get_text(strip=True)) for u in ubi]\n",
    "\n",
    "                distrito2 = next((i for i in ubicacion_items if 'Distrito' in i), None)\n",
    "                \n",
    "                barrio2 = next((i for i in ubicacion_items if 'Barrio' in i), None)\n",
    "\n",
    "                calle = ubicacion_items[0]\n",
    "                \n",
    "                barrio = ubicacion_items[1]\n",
    "\n",
    "                distrito = ubicacion_items[2]\n",
    "         \n",
    "                area = ubicacion_items[4]\n",
    "                precio = soup.select_one('.info-data span').get_text(strip=True).strip(' €/mes')\n",
    "\n",
    "                if \".\" in precio:\n",
    "                    precio = precio.replace('.', '')  # falta formato int\n",
    "\n",
    "                precio_down = soup.select_one(\".pricedown_price\")\n",
    "                  \n",
    "                if precio_down != None :\n",
    "                    precio_down = precio_down.text.strip(\"\\n\").strip(\" €\").replace('.','')\n",
    "                else:\n",
    "                    precio_down = precio\n",
    "                \n",
    "#                 detalles basicos del piso habitaciones, baños, etc\n",
    "                detalles = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-one\"})\n",
    "                # info sobre aire acondicionado, psicina, zonas verdes y consumo energetico\n",
    "                detalles2 = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-two\"})\n",
    "                caract = [det.text.strip() for det in detalles.find_all(\"li\")]\n",
    "\n",
    "                s = ' '.join(caract).lower()\n",
    "\n",
    "                metros = regex_m2.search(s).group().replace(' m²','')\n",
    "\n",
    "                hab =  regex_hab.search(s).group()\n",
    "\n",
    "                wc =  regex_baño.search(s).group()\n",
    "        \n",
    "                terraza = \"\".join(re.findall(r'terraza',s))\n",
    "                balcon = \"\".join(re.findall(r'balcón|balcon',s))\n",
    "                estado = \"\".join(re.findall(r'segunda mano/buen estado',s))\n",
    "                year = \"\".join(re.findall(r'\\d\\d\\d\\d',s))\n",
    "                armarios = \"\".join(re.findall(r'armarios',s))\n",
    "                cocina = \"\".join(re.findall(r'cocina equipada|cocina sin equipar',s))\n",
    "                amu = \"\".join(re.findall(r'amueblada|amueblado|sin amueblar',s))\n",
    "\n",
    "                if regex_planta.search(s) == None:\n",
    "                    planta = 0\n",
    "                else:\n",
    "                    planta = regex_planta.search(s).group()\n",
    "    \n",
    "                calefac = \"\".join(re.findall(r'no dispone de calefacción|calefacción',s))\n",
    "                ascn = \"\".join(re.findall(r'con ascensor|sin ascensor',s))\n",
    "                aire = detalles2.find('ul').text.strip(\"\\n\").lower()\n",
    "                \n",
    "                if aire == \"aire acondicionado\":\n",
    "                    aire = 1\n",
    "                else:\n",
    "                    aire = 0\n",
    "                exterior = \"\".join(re.findall(r'exterior|interior',s))\n",
    "                datalles2 = \"\".join([det.text.replace(\"\\n\",\"|\").lower() for det  in detalles2.find_all('ul')]) # añado zonas verdes, psicina\n",
    "                cp = cp\n",
    "                actualizacion = soup.find('p',{\"class\":\"date-update-text\"}).text\n",
    "  \n",
    "                actualizacion2 = soup.find(\"p\", {\"class\":\"stats-text\"}).text\n",
    "                extract_day = date.today()\n",
    "\n",
    "                dic[i] = [titulo,localizacion,ubicacion_full,distrito2,calle,barrio,barrio2,distrito,area,precio,precio_down,metros,hab,wc,terraza,balcon,estado,year,armarios,cocina,amu,planta,calefac,ascn,aire,exterior,datalles2,cp,actualizacion,actualizacion2,extract_day]\n",
    "                   \n",
    "                len_ids = len(ids)\n",
    "                print(r, \"anuncio número:{}/{} cp: {}\".format(x,len_ids,cp),sep=\"\\n\")\n",
    "                \n",
    "                x += 1\n",
    "                \n",
    "                time.sleep(random.randint(3,4)*random.random())\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"error {e}\")\n",
    "        \n",
    "#                 dic[i] = [\"error\"] * 50\n",
    "                          \n",
    "    return dic\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_to_df(dic):\n",
    "    '''\n",
    "    Transform dic from Idealista https://www.idealista.com/inmueble\n",
    "    to a data frame.\n",
    "    \n",
    "    dic -> Dataframe\n",
    "    '''\n",
    "    colnames = ['name','zone','ubicacion_full','distrito2','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df = pd.DataFrame.from_dict(dic, orient='index',columns=colnames)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_parse(df):\n",
    "    '''\n",
    "    Transform dataframe from Idealista https://www.idealista.com/inmueble\n",
    "    to a clean data frame and convert the data types to int when possible.\n",
    "    \n",
    "    Dataframe -> Dataframe\n",
    "    '''\n",
    "    df['price'] = df.price.astype(int)\n",
    "    df['price_before'] = df.price_before.astype(int)\n",
    "    df['square_mt'] = df.square_mt.astype(int)\n",
    "    try:\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace('sin habitación','0'))\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df ['wc']=df.wc.apply(lambda x: x.replace(' baño','')).astype(int)\n",
    "    #     df ['wc']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['terraza'] = df.terraza.apply(lambda x:  x.replace('terraza','1') if x == 'terraza' else '0').astype(int)\n",
    "    df['balcon'] = df.balcon.apply(lambda x:  x.replace('balcón','1') if x == 'balcón' else '0').astype(int)\n",
    "    df['armarios'] = df.armarios.apply(lambda x:  x.replace('armarios','1') if x == 'armarios' else '0').astype(int)\n",
    "    df['cocina'] = df.cocina.apply(lambda x:  x.replace('cocina equipada','1') if x == 'cocina equipada' else '0').astype(int)\n",
    "    df['amueblado'] = df.amueblado.apply(lambda x:  x.replace('amueblado','1') if x == 'amueblado' else '0').astype(int)\n",
    "    df['calef'] = df.calef.apply(lambda x:  x.replace('calefacción','1') if x == 'calefacción' else '0').astype(int)\n",
    "    df['asc'] = df.asc.apply(lambda x:  x.replace('con ascensor','1') if x == 'con ascensor' else '0').astype(int)\n",
    "    df['aire'] = df.aire.astype(int)\n",
    "    df['exterior'] = df.exterior.apply(lambda x:  x.replace('exterior','1') if x == 'exterior' else '0').astype(int)\n",
    "    try:\n",
    "        df['planta'] = df.planta.apply(lambda x: x.replace('planta ',''))\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"\" else x)\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"bajo\" else x)\n",
    "        df['planta'] = df.planta.astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['cp'] = df.cp.astype(str)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista de codigos postales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08001',\n",
       " '08002',\n",
       " '08003',\n",
       " '08004',\n",
       " '08005',\n",
       " '08006',\n",
       " '08007',\n",
       " '08008',\n",
       " '08009',\n",
       " '08010',\n",
       " '08011',\n",
       " '08012',\n",
       " '08013',\n",
       " '08014',\n",
       " '08015',\n",
       " '08016',\n",
       " '08017',\n",
       " '08018',\n",
       " '08019',\n",
       " '08020',\n",
       " '08021',\n",
       " '08022',\n",
       " '08023',\n",
       " '08024',\n",
       " '08025',\n",
       " '08026',\n",
       " '08027',\n",
       " '08028',\n",
       " '08029',\n",
       " '08030',\n",
       " '08031',\n",
       " '08032',\n",
       " '08033',\n",
       " '08034',\n",
       " '08035',\n",
       " '08036',\n",
       " '08037',\n",
       " '08038',\n",
       " '08039',\n",
       " '08040',\n",
       " '08041',\n",
       " '08042']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = [\"{:05d}\".format(i) for i in range(8001,8043)] # 8001 - 8010 , 8011 - 8021, 8021 - 8031\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cp = \"https://www.idealista.com/buscar/alquiler-viviendas/{}/\".format(cp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.idealista.com/buscar/alquiler-viviendas/08017/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent_path = 'C:\\\\Users\\\\ggari\\\\Desktop\\\\1_projects\\\\TFM'\n",
    "\n",
    "data_idealista_path = 'C:\\\\Users\\\\ggari\\\\Desktop\\\\1_projects\\\\TFM\\\\1_data\\\\2_data_Idealista\\\\1_raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo un nuevo dataframe para juntar todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(parent_path):\n",
    "    \n",
    "    today = date.today()\n",
    "    directory = \"extraction_{}\".format(today)\n",
    "    \n",
    "    list_files = os.listdir(parent_path)\n",
    "#     path = os.path.join(parent_path, directory)\n",
    "    \n",
    "    if directory not in list_files:\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        os.mkdir(path)\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' created\" % directory)\n",
    "    else:\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' already created\" % directory)\n",
    "    \n",
    "    return directory\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_scrapper(cp_list,parent_path):\n",
    "    \n",
    "    print(\"parent directory: \",os.getcwd())\n",
    "    \n",
    "    new_dir_name = create_folder(parent_path)\n",
    "    \n",
    "    os.chdir(\"{}\\\\{}\".format(parent_path,new_dir_name))\n",
    "\n",
    "    print(\"csv folder: \",os.getcwd())\n",
    "    \n",
    "    for chunked_list in chunk(cp_list, 6):\n",
    "        tmp_cp_list = chunked_list\n",
    "        print(tmp_cp_list)\n",
    "    \n",
    "        for ind, zip_code in enumerate(tmp_cp_list):\n",
    "            print(\"buscando:\",zip_code)\n",
    "            id_from_cp = get_ids(headers, zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "            dic_data = obtain_data_from_ids(id_from_cp,headers,cp = zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "            df_data = df_parse(dic_to_df(dic_data))\n",
    "\n",
    "            df_data.to_csv(\"data_{}.csv\".format(zip_code),encoding = 'utf-8-sig',index_label= \"id\")\n",
    "    #         df_final = df_final.append(df_data)\n",
    "\n",
    "        #     cp.pop(0) da problemas\n",
    "        #     Para saber que cp han sido buscados se agregan a cp_buscados\n",
    "\n",
    "    #         cp_buscados.append(zip_code)\n",
    "\n",
    "            print(\"guardado datos de:\",zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random()) \n",
    "\n",
    "        time.sleep(random.randint(750,900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv(parent_path, folder=None):\n",
    "    \n",
    "    colnames = ['id','name','zone','ubicacion_full','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df_final = pd.DataFrame(columns = colnames)\n",
    "    \n",
    "#     today = date.today()\n",
    "    \n",
    "    if folder is None:\n",
    "        today = date.today()\n",
    "        directory = \"extraction_{}\".format(today)\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "            csv = os.listdir()\n",
    "        except ValueError:\n",
    "            print(\"path does not exists\")\n",
    "\n",
    "        \n",
    "    \n",
    "    if folder is not None:\n",
    "\n",
    "        path = os.path.join(parent_path, folder)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "            csv = os.listdir()\n",
    "        except ValueError:\n",
    "            print(\"path does not exists\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    for i in csv:\n",
    "        df = pd.read_csv(i)\n",
    "        df_final = pd.concat([df_final,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    df_final.to_csv(\"datos_scrapping_{}.csv\".format(today),encoding = 'utf-8-sig',index=False)\n",
    "    print(\"file '% s' created\" % \"datos_scrapping_{}.csv\".format(today))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # call_scrapper(cp,data_idealista_path)\n",
    "# call_scrapper(cp,data_idealista_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists\n",
      "file 'datos_scrapping_2023-05-20.csv' created\n"
     ]
    }
   ],
   "source": [
    "concat_csv(data_idealista_path,folder=\"extraction_2023-05-30\")\n",
    "# parent_path\n",
    "# data_idealista_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"extraction_2023-04-11\"\n",
    "# re.search('\\d{4}-\\d{2}-\\d{2}', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
