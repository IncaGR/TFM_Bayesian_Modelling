{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers -> Ctrl + shift + i (pagina de desarrollador) -> network Ctrl + f5 -> primera pestaña -> request headers\n",
    "# desde \"accept\" ponerlo en dict \n",
    "headers = {\n",
    "\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\"accept-encoding\": \"gzip, deflate, br\",\n",
    "\"accept-language\": \"en-GB,en;q=0.9,en-US;q=0.8,ca;q=0.7,es;q=0.6,eo;q=0.5\",\n",
    "\"cache-control\": \"no-cache\",\n",
    "\"cookie\": '''atuserid=%7B%22name%22%3A%22atuserid%22%2C%22val%22%3A%2268f673fe-430e-424d-9df0-0cd8699a34a7%22%2C%22options%22%3A%7B%22end%22%3A%222023-03-03T18%3A46%3A49.424Z%22%2C%22path%22%3A%22%2F%22%7D%7D; atidvisitor=%7B%22name%22%3A%22atidvisitor%22%2C%22val%22%3A%7B%22vrn%22%3A%22-582065-%22%7D%2C%22options%22%3A%7B%22path%22%3A%22%2F%22%2C%22session%22%3A15724800%2C%22end%22%3A15724800%7D%7D; didomi_token=eyJ1c2VyX2lkIjoiMTdlYWM0ZmEtNzcxNC02MGMyLWJmODMtYWRjMGViMzFiYzE3IiwiY3JlYXRlZCI6IjIwMjItMDEtMzBUMTg6NDY6NTEuMTA0WiIsInVwZGF0ZWQiOiIyMDIyLTAxLTMwVDE4OjQ2OjUxLjEwNFoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsiZ29vZ2xlIiwiYzptaXhwYW5lbCIsImM6YWJ0YXN0eS1MTGtFQ0NqOCIsImM6aG90amFyIiwiYzp5YW5kZXhtZXRyaWNzIiwiYzpiZWFtZXItSDd0cjdIaXgiLCJjOmFwcHNmbHllci1HVVZQTHBZWSIsImM6dGVhbGl1bWNvLURWRENkOFpQIiwiYzppZGVhbGlzdGEtTHp0QmVxRTMiLCJjOmlkZWFsaXN0YS1mZVJFamUyYyJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJhbmFseXRpY3MtSHBCSnJySzciLCJnZW9sb2NhdGlvbl9kYXRhIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkFGbUFDQUZrLkFBQUEifQ==; euconsent-v2=CPTmlIAPTmlIAAHABBENB_CoAP_AAAAAAAAAF5wBAAIAAtAC2AvMAAABAaADAAEEQyUAGAAIIhlIAMAAQRDIQAYAAgiGOgAwABBEMJABgACCIYyADAAEEQxUAGAAIIhg.f_gAAAAAAAAA; _gcl_au=1.1.8146483.1643568415; _fbp=fb.1.1643568415299.2070506742; afUserId=58c2a54c-4375-4eb4-baca-6a6e48c65725-p; AF_SYNC=1643568416416; askToSaveAlertPopUp=true; _hjSessionUser_250321=eyJpZCI6ImU0N2FkYjkyLTgyMDMtNWU0Mi1iMTRiLWIzMzQzMDE0NTM0NiIsImNyZWF0ZWQiOjE2NDM1Njg1MjgzMzYsImV4aXN0aW5nIjp0cnVlfQ==; userUUID=6eadbb28-4981-4c13-abda-fd7fadaec832; SESSION=f1f80941eb4fade2~bda569d4-9873-4a74-80a2-5fd0ae1a6c25; contactbda569d4-9873-4a74-80a2-5fd0ae1a6c25=\"{'email':null,'phone':null,'phonePrefix':null,'friendEmails':null,'name':null,'message':null,'message2Friends':null,'maxNumberContactsAllow':10,'defaultMessage':true}\"; cookieSearch-1=\"/alquiler-viviendas/barcelona/les-corts/les-corts/:1643650811421\"; sendbda569d4-9873-4a74-80a2-5fd0ae1a6c25=\"{'friendsEmail':null,'email':null,'message':null}\"; _hjIncludedInSessionSample=1; _hjSession_250321=eyJpZCI6IjYyNTE2ZTJjLWNkNTYtNDRkZS05NmJkLWQ4ZjYxYjQwMGE1NCIsImNyZWF0ZWQiOjE2NDM2NTEzNjU5MTEsImluU2FtcGxlIjp0cnVlfQ==; _hjAbsoluteSessionInProgress=0; _hjCachedUserAttributes=eyJhdHRyaWJ1dGVzIjp7ImlkX3BhZ2VMYW5ndWFnZSI6ImVzIiwiaWRfdXNlclJvbGUiOiIifSwidXNlcklkIjpudWxsfQ==; ABTasty=uid=s43fcyc0hr2z8nkz&fst=1643568412317&pst=1643568412317&cst=1643650069991&ns=2&pvt=35&pvis=13&th=; ABTastySession=mrasn=&sen=12&lp=https%253A%252F%252Fwww.idealista.com%252F; utag_main=v_id:017eac27920f001f94d41f93932f05072002f06a00978$_sn:3$_se:17$_ss:0$_st:1643653573939$dc_visit:2$ses_id:1643650066499%3Bexp-session$_pn:13%3Bexp-session$_prevVtSource:directTraffic%3Bexp-1643653668218$_prevVtCampaignCode:%3Bexp-1643653668218$_prevVtDomainReferrer:%3Bexp-1643653668218$_prevVtSubdomaninReferrer:%3Bexp-1643653668218$_prevVtUrlReferrer:%3Bexp-1643653668218$_prevVtCampaignLinkName:%3Bexp-1643653668218$_prevVtCampaignName:%3Bexp-1643653668218$_prevVtRecommendationId:%3Bexp-1643653668218$_prevCompletePageName:12%3A%3Adetail%3A%3A%3A%3A%3A%3Ahome%3Bexp-1643655375478$_prevLevel2:12%3Bexp-1643655375478$_prevAdId:94476328%3Bexp-1643655375489$_prevAdOriginTypeRecommended:undefined%3Bexp-1643653668228$dc_event:5%3Bexp-session$dc_region:us-east-1%3Bexp-session; cto_bundle=5izo9l9XUkgzNEJIYU9ITGFNWU5xMWp4YWoxNCUyRnFJN2dHbFJTbWRCVlpldHRFbjF4aFFaSmFvWHZvV3B2WCUyRmx1MEhiMUNQbDlUQmlPckkwdW51VTdTZjltc255c0ZjRng0d1c4MXVFVURBdUFXdEYwaSUyQnhtSlowY3oybzE4TkxtY2RVWmR3ZHJvbVJvUko1MmQxT1FJczJvV2clM0QlM0Q; datadome=v475FOZILU7oISIcQM..EhoU-BUZiH-ZFK2rcgXnvpb4HM05afYuAyFXtJCwx2Wz7bUmxBeuhegZikjdshl3ZtuSvzBOApWI_EnGCatwDUdNRGdY_Ox0oK6H5CgXTyT''',\n",
    "\"pragma\": \"no-cache\",\n",
    "\"referer\": \"https://www.idealista.com/inmueble/95249555/\",\n",
    "\"sec-ch-ua\": '\" Not;A Brand\";v=\"99\", \"Google Chrome\";v=\"97\", \"Chromium\";v=\"97\"',\n",
    "\"sec-ch-ua-mobile\": \"?0\",\n",
    "\"sec-ch-ua-platform\": '\"Windows\"',\n",
    "\"sec-fetch-dest\": \"document\",\n",
    "\"sec-fetch-mode\": \"navigate\",\n",
    "\"sec-fetch-site\": \"same-origin\",\n",
    "\"sec-fetch-user\": \"?1\",\n",
    "\"upgrade-insecure-requests\": \"1\",\n",
    "\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\"\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# headers = {'User-agent': 'your bot 0.1'} # no funciona necesita datos reales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion usar ids y devolver caracteristicas inmueble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(headers,cp):\n",
    "    '''\n",
    "    Function that returns all the flats ids of certain zip code with fixed headers request.\n",
    "    The function return a list with the ids.\n",
    "    \n",
    "    headers: needed to obtain response 200 (success).\n",
    "    cp: zip code.\n",
    "    proxies: proxy to try.\n",
    "    \n",
    "    Returns list of ids.\n",
    "    '''\n",
    "    cp = cp\n",
    "    x = 2 # 1\n",
    "    ids = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # nueva linea tratando los casos donde hay solo una pagina de anuncios.\n",
    "        \n",
    "        url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/'\n",
    "        r = requests.get(url, headers = headers)\n",
    "        soup = bs(r.text,'lxml')\n",
    "        \n",
    "        articles = soup.find('main', {'class':'listing-items'}).find_all('article')\n",
    "        \n",
    "        for article in articles:\n",
    "            id_piso = article.get('data-adid')  # get()  return the value of the key\n",
    "            ids.append(id_piso)\n",
    "        \n",
    "#         time.sleep(random.randint(1,3)*random.random())\n",
    "            time.sleep(random.randint(1,2)*random.random())\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/pagina-{x}.htm'\n",
    "            r = requests.get(url, headers = headers)\n",
    "            soup = bs(r.text,'lxml')\n",
    "\n",
    "            pag_actual = int(soup.find('main', {'class':'listing-items'}).find('div',{'class':'pagination'}).find('li',{'class':'selected'}).text)\n",
    "\n",
    "            if x == pag_actual:\n",
    "                articles = soup.find('main', {'class':'listing-items'}).find_all('article')\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                id_piso = article.get('data-adid')  # get()  return the value of the key\n",
    "                ids.append(id_piso)\n",
    "\n",
    "        #     id_pisos =  [article.get('data-adid') for article in articles]  # get()  return the value of the key\n",
    "                time.sleep(random.randint(1,2)*random.random()) #\n",
    "        #     ids.append(id_pisos)\n",
    "\n",
    "            x += 1\n",
    "\n",
    "    except:\n",
    "\n",
    "        print(r)\n",
    "        \n",
    "    return ids\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data_from_ids(ids,headers,cp):\n",
    "    '''\n",
    "    Function that return all the data from each add id from previous function \"get_ids\".\n",
    "    It returns the data in dictionary format, ready to be converted to dataframe.\n",
    "    \n",
    "    ids = list of ids.\n",
    "    headers = to avoid errors, response 200 (success)\n",
    "    cp = zip code of the ids\n",
    "    \n",
    "    Returns a dic with id as a key.\n",
    "    '''\n",
    "    \n",
    "    dic = {}\n",
    "    x=1\n",
    "    \n",
    "\n",
    "    for i in ids:\n",
    "        print(\"aqui entra en el bucle\")\n",
    "        try:\n",
    "            \n",
    "            print(\"aqui esta en el try\")\n",
    "        \n",
    "            url = f\"https://www.idealista.com/inmueble/{i}/\"\n",
    "            \n",
    "            print(\"id:\",i)\n",
    "\n",
    "            r = requests.get(url,headers=headers) # añadimos parametro headers\n",
    "#             print(r)\n",
    "            soup = bs(r.text,'lxml')  # formato mas legible\n",
    "#             print(soup)\n",
    "            # ubicacion\n",
    "            ubi = soup.find(\"div\",{\"id\":\"headerMap\"}).find_all(\"li\",)  # lista\n",
    "            \n",
    "            ubicacion_full = [u.text.replace(\"\\n\",\"\") for u in ubi] # ubicacion completa\n",
    "            ubicacion_full = '||'.join(ubicacion_full) # string\n",
    "    \n",
    "            \n",
    "            titulo = soup.find('h1').text.strip() \n",
    "#             comentario = soup.find(\"div\",{\"class\":\"comment\"}).text.replace('\\n','')\n",
    "            localizacion = soup.find('span',{'class':'main-info__title-block'}).find(\"span\").text\n",
    "            # ubi table\n",
    "            distrito2 = \"\".join(re.findall(r\"Distrito[^|]*\",ubicacion_full))\n",
    "            barrio2 = \"\".join(re.findall(r\"Barrio[^|]*\",ubicacion_full))\n",
    "            calle = ubi[0].text.replace(\"\\n\",\"\")\n",
    "            barrio = ubi[1].text.replace(\"\\n\",\"\")\n",
    "            distrito = ubi[2].text.replace(\"\\n\",\"\")\n",
    "            area = ubi[4].text.replace(\"\\n\",\"\")\n",
    "            \n",
    "            precio = soup.find('div',{'class':'info-data'}).find('span').text.strip(' €/mes')\n",
    "            \n",
    "#             print(\"hasta aqui? precio\")\n",
    "            \n",
    "            if \".\" in precio:\n",
    "                precio = precio.replace('.','') # falta formato int\n",
    "\n",
    "            precio_down = soup.find(\"span\",{\"class\":\"pricedown_price\"})\n",
    "            \n",
    "#             print(\"hasta aqui? precio down\")\n",
    "\n",
    "            if precio_down != None :\n",
    "                precio_down = precio_down.text.strip(\"\\n\").strip(\" €\").replace('.','')\n",
    "            else:\n",
    "                precio_down = precio\n",
    "                #########\n",
    "                \n",
    "#             print(\"hasta aqui? precio down 2\")\n",
    "            \n",
    "#             bug duplicate price down \n",
    "            \n",
    "#             precio = soup.find('div',{'class':'info-data'}).find('span').text.strip(' €/mes')\n",
    "#             if \".\" in precio:\n",
    "#                 precio = precio.replace('.','') # falta formato int\n",
    "\n",
    "#                 precio_down = soup.find(\"span\",{\"class\":\"pricedown_price\"})\n",
    "                \n",
    "#             print(\"hasta aqui? precio down 3?\")\n",
    "\n",
    "#             if precio_down != None :\n",
    "#                 precio_down = precio_down.text.strip(\"\\n\").strip(\" €\").replace('.','')\n",
    "#             else:\n",
    "#                 precio_down = precio\n",
    "                \n",
    "#             print(\"hasta aqui? precio down 4?\")\n",
    "#             print(\"hasta aqui?\")\n",
    "###############################\n",
    "            # detalles basicos del piso habitaciones, baños, etc\n",
    "            detalles = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-one\"})\n",
    "            # info sobre aire acondicionado, psicina, zonas verdes y consumo energetico\n",
    "            detalles2 = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-two\"})\n",
    "            caract = [det.text.strip() for det in detalles.find_all(\"li\")]\n",
    "\n",
    "            s = ' '.join(caract).lower()\n",
    "\n",
    "            metros = re.search(r'\\d\\d\\d m²|\\d\\d m²',s).group().replace(' m²','')\n",
    "            # util = re.search(r'\\d\\d m² útiles',s).group()\n",
    "            hab =  \"\".join(re.findall(r'\\d hab|sin habitación',s))\n",
    "            wc =  \"\".join(re.findall(r'\\d baño',s))\n",
    "            terraza = \"\".join(re.findall(r'terraza',s))\n",
    "            balcon = \"\".join(re.findall(r'balcón|balcon',s))\n",
    "            estado = \"\".join(re.findall(r'segunda mano/buen estado',s))\n",
    "            year = \"\".join(re.findall(r'\\d\\d\\d\\d',s))\n",
    "            armarios = \"\".join(re.findall(r'armarios',s))\n",
    "            cocina = \"\".join(re.findall(r'cocina equipada|cocina sin equipar',s))\n",
    "            amu = \"\".join(re.findall(r'amueblada|amueblado|sin amueblar',s))\n",
    "            # noAmu = re.findall(r'sin amueblar',s)\n",
    "            planta = \"\".join(re.findall(r'planta \\d\\d|planta \\d|bajo',s))\n",
    "            calefac = \"\".join(re.findall(r'no dispone de calefacción|calefacción',s))\n",
    "            ascn = \"\".join(re.findall(r'con ascensor|sin ascensor',s))\n",
    "            aire = detalles2.find('ul').text.strip(\"\\n\").lower()\n",
    "            if aire == \"aire acondicionado\":\n",
    "                aire = 1\n",
    "            else:\n",
    "                aire = 0\n",
    "            exterior = \"\".join(re.findall(r'exterior|interior',s))\n",
    "            datalles2 = \"\".join([det.text.replace(\"\\n\",\"|\").lower() for det  in detalles2.find_all('ul')]) # añado zonas verdes, psicina\n",
    "            cp = cp\n",
    "            actualizacion = soup.find('p',{\"class\":\"date-update-text\"}).text\n",
    "            #     aire = \"\".join(re.findall(r'aire acondicionado',s))\n",
    "            actualizacion2 = soup.find(\"p\", {\"class\":\"stats-text\"}).text\n",
    "            extract_day = date.today()\n",
    "#             soup_save = soup\n",
    "#             print(\"hasta aqui?\")\n",
    "\n",
    "#             print(\"aqui empieza el dic\",titulo)\n",
    "#             print(\"aqui empieza el dic\",comentario)\n",
    "#             print(\"aqui empieza el dic\",localizacion)\n",
    "#             print(\"aqui empieza el dic\",ubicacion_full)\n",
    "#             print(\"aqui empieza el dic\",distrito2)\n",
    "#             print(\"aqui empieza el dic\",calle)\n",
    "#             print(\"aqui empieza el dic\",barrio)\n",
    "#             print(\"aqui empieza el dic\",distrito)\n",
    "#             print(\"aqui empieza el dic\",area)\n",
    "#             print(\"aqui empieza el dic\",precio)\n",
    "#             print(\"aqui empieza el dic\",precio_down)\n",
    "#             print(\"aqui empieza el dic\",metros)\n",
    "#             print(\"aqui empieza el dic\",hab)\n",
    "#             print(\"aqui empieza el dic\",wc)\n",
    "#             print(\"aqui empieza el dic\",terraza)\n",
    "#             print(\"aqui empieza el dic\",balcon)\n",
    "#             print(\"aqui empieza el dic\",estado)\n",
    "#             print(\"aqui empieza el dic\",year)\n",
    "#             print(\"aqui empieza el dic\",armarios)\n",
    "#             print(\"aqui empieza el dic\",cocina)\n",
    "#             print(\"aqui empieza el dic\",amu)\n",
    "#             print(\"aqui empieza el dic\",planta)\n",
    "#             print(\"aqui empieza el dic\",calefac)\n",
    "#             print(\"aqui empieza el dic\",ascn)\n",
    "#             print(\"aqui empieza el dic\",aire)\n",
    "#             print(\"aqui empieza el dic\",exterior)\n",
    "#             print(\"aqui empieza el dic\",datalles2)\n",
    "#             print(\"aqui empieza el dic\",cp)\n",
    "#             print(\"aqui empieza el dic\",actualizacion)\n",
    "#             print(\"aqui empieza el dic\",actualizacion2)\n",
    "#             print(\"aqui empieza el dic\",extract_day)\n",
    "\n",
    "            dic[i] = [titulo,localizacion,ubicacion_full,distrito2,calle,barrio,barrio2,distrito,area,precio,precio_down,metros,hab,wc,terraza,balcon,estado,year,armarios,cocina,amu,planta,calefac,ascn,aire,exterior,datalles2,cp,actualizacion,actualizacion2,extract_day]\n",
    "#             print(ubicacion_full)\n",
    "#             print(dic[i])\n",
    "#             print(precio)            \n",
    "            len_ids = len(ids)\n",
    "            print(r, \"anuncio número:{}/{} cp: {}\".format(x,len_ids,cp),sep=\"\\n\")\n",
    "            \n",
    "            x += 1\n",
    "            \n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "        \n",
    "        except:\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>> error 404?, id \",i,r)\n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "        \n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_to_df(dic):\n",
    "    '''\n",
    "    Transform dic from Idealista https://www.idealista.com/inmueble\n",
    "    to a data frame.\n",
    "    \n",
    "    dic -> Dataframe\n",
    "    '''\n",
    "    colnames = ['name','zone','ubicacion_full','distrito2','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df = pd.DataFrame.from_dict(dic, orient='index',columns=colnames)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_parse(df):\n",
    "    '''\n",
    "    Transform dataframe from Idealista https://www.idealista.com/inmueble\n",
    "    to a clean data frame and convert the data types to int when possible.\n",
    "    \n",
    "    Dataframe -> Dataframe\n",
    "    '''\n",
    "    df['price'] = df.price.astype(int)\n",
    "    df['price_before'] = df.price_before.astype(int)\n",
    "    df['square_mt'] = df.square_mt.astype(int)\n",
    "    try:\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace('sin habitación','0'))\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df ['wc']=df.wc.apply(lambda x: x.replace(' baño','')).astype(int)\n",
    "    #     df ['wc']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['terraza'] = df.terraza.apply(lambda x:  x.replace('terraza','1') if x == 'terraza' else '0').astype(int)\n",
    "    df['balcon'] = df.balcon.apply(lambda x:  x.replace('balcón','1') if x == 'balcón' else '0').astype(int)\n",
    "    df['armarios'] = df.armarios.apply(lambda x:  x.replace('armarios','1') if x == 'armarios' else '0').astype(int)\n",
    "    df['cocina'] = df.cocina.apply(lambda x:  x.replace('cocina equipada','1') if x == 'cocina equipada' else '0').astype(int)\n",
    "    df['amueblado'] = df.amueblado.apply(lambda x:  x.replace('amueblado','1') if x == 'amueblado' else '0').astype(int)\n",
    "    df['calef'] = df.calef.apply(lambda x:  x.replace('calefacción','1') if x == 'calefacción' else '0').astype(int)\n",
    "    df['asc'] = df.asc.apply(lambda x:  x.replace('con ascensor','1') if x == 'con ascensor' else '0').astype(int)\n",
    "    df['aire'] = df.aire.astype(int)\n",
    "    df['exterior'] = df.exterior.apply(lambda x:  x.replace('exterior','1') if x == 'exterior' else '0').astype(int)\n",
    "    try:\n",
    "        df['planta'] = df.planta.apply(lambda x: x.replace('planta ',''))\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"\" else x)\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"bajo\" else x)\n",
    "        df['planta'] = df.planta.astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['cp'] = df.cp.astype(str)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba de una sola id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # prueba con una sola id\n",
    "# dic = {}\n",
    "# cp = \"08039\" # barceloneta  ids\n",
    "# i = \"83328665\"\n",
    "# url = f\"https://www.idealista.com/inmueble/{i}/\"\n",
    "\n",
    "# print(\"id:\",i)\n",
    "\n",
    "# r = requests.get(url,headers=headers) # añadimos parametro headers\n",
    "# print(\"respuests: \",r)\n",
    "# soup = bs(r.text,'lxml')  # formato mas legible\n",
    "\n",
    "# titulo = soup.find('h1').text.strip() \n",
    "# # comentario = soup.find(\"div\",{\"class\":\"comment\"}).text.replace('\\n','')\n",
    "# localizacion = soup.find('span',{'class':'main-info__title-block'}).find(\"span\").text\n",
    "# precio = soup.find('div',{'class':'info-data'}).find('span').text.strip(' €/mes')\n",
    "# if \".\" in precio:\n",
    "#     precio = precio.replace('.','') # falta formato int\n",
    "    \n",
    "# precio_down = soup.find(\"span\",{\"class\":\"pricedown_price\"})\n",
    "\n",
    "# if precio_down != None :\n",
    "#     precio_down = precio_down.text.strip(\"\\n\").strip(\" €\").replace('.','')\n",
    "# else:\n",
    "#     precio_down = precio\n",
    "\n",
    "# # detalles basicos del piso habitaciones, baños, etc\n",
    "# detalles = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-one\"})\n",
    "# # info sobre aire acondicionado, psicina, zonas verdes y consumo energetico\n",
    "# detalles2 = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-two\"})\n",
    "# caract = [det.text.strip() for det in detalles.find_all(\"li\")]\n",
    "\n",
    "# s = ' '.join(caract).lower()\n",
    "\n",
    "# metros = re.search(r'\\d\\d\\d m²|\\d\\d m²',s).group().replace(' m²','')\n",
    "# # util = re.search(r'\\d\\d m² útiles',s).group()\n",
    "# hab =  \"\".join(re.findall(r'\\d hab|sin habitación',s))\n",
    "# wc =  \"\".join(re.findall(r'\\d baño',s))\n",
    "# terraza = \"\".join(re.findall(r'terraza',s))\n",
    "# balcon = \"\".join(re.findall(r'balcón|balcon',s))\n",
    "# estado = \"\".join(re.findall(r'segunda mano/buen estado',s))\n",
    "# year = \"\".join(re.findall(r'\\d\\d\\d\\d',s))\n",
    "# armarios = \"\".join(re.findall(r'armarios',s))\n",
    "# cocina = \"\".join(re.findall(r'cocina equipada|cocina sin equipar',s))\n",
    "# amu = \"\".join(re.findall(r'amueblada|amueblado|sin amueblar',s))\n",
    "# # noAmu = re.findall(r'sin amueblar',s)\n",
    "# planta = \"\".join(re.findall(r'planta \\d\\d|planta \\d|bajo',s))\n",
    "# calefac = \"\".join(re.findall(r'no dispone de calefacción|calefacción',s))\n",
    "# ascn = \"\".join(re.findall(r'con ascensor|sin ascensor',s))\n",
    "# aire = detalles2.find('ul').text.strip(\"\\n\").lower()\n",
    "# if aire == \"aire acondicionado\":\n",
    "#     aire = 1\n",
    "# else:\n",
    "#     aire = 0\n",
    "# exterior = \"\".join(re.findall(r'exterior|interior',s))\n",
    "# datalles2 = \"\".join([det.text.replace(\"\\n\",\"|\").lower() for det  in detalles2.find_all('ul')]) # añado zonas verdes, psicina\n",
    "# cp = cp\n",
    "# actualizacion = soup.find('p',{\"class\":\"date-update-text\"}).text\n",
    "# #     aire = \"\".join(re.findall(r'aire acondicionado',s))\n",
    "# actualizacion2 = soup.find(\"p\", {\"class\":\"stats-text\"}).text\n",
    "# extract_day = date.today()\n",
    "# # soup_save = soup\n",
    "\n",
    "# ubi = soup.find(\"div\",{\"id\":\"headerMap\"}).find_all(\"li\",)\n",
    "# ubicacion_full = [u.text.replace(\"\\n\",\"\") for u in ubi]\n",
    "# ubicacion_full = '||'.join(ubicacion_full)\n",
    "\n",
    "# distrito = \"\".join(re.findall(r\"Distrito[^|]*\",ubicacion_full))\n",
    "# barrio = \"\".join(re.findall(r\"Barrio[^|]*\",ubicacion_full))\n",
    "\n",
    "# barrio2 = \"\".join(re.findall(r\"Barrio[^|]*\",ubicacion_full))\n",
    "\n",
    "# dic[i] = [titulo,localizacion,precio,precio_down,metros,hab,wc,terraza,balcon,estado,year,armarios,cocina,amu,planta,calefac,ascn,aire,exterior,cp,actualizacion,extract_day,barrio2]\n",
    "\n",
    "\n",
    "# # print(dic)\n",
    "# # print(soup)\n",
    "\n",
    "# # var utag_data\n",
    "\n",
    "# # print(soup.find('h1').text.strip()) \n",
    "\n",
    "# # soup.find_all('script')\n",
    "# # print(soup.find('body class=\"\" id=\"\"')) \n",
    "\n",
    "# dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obtain_data_from_ids(get_ids(headers,'08039'),headers,['08039'])\n",
    "# obtain_data_from_ids(['99072099'],headers,['08039'])\n",
    "# # obtain_data_from_ids algo esta mal aqui\n",
    "# # get_ids(headers,'08039')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista de codigos postales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08001',\n",
       " '08002',\n",
       " '08003',\n",
       " '08004',\n",
       " '08005',\n",
       " '08006',\n",
       " '08007',\n",
       " '08008',\n",
       " '08009',\n",
       " '08010']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cp = [\"{:05d}\".format(i) for i in range(8016,8017)]\n",
    "# cp = [\"{:05d}\".format(i) for i in range(8001,8011)]\n",
    "# cp = [\"{:05d}\".format(i) for i in range(8011,8021)]\n",
    "# cp = [\"{:05d}\".format(i) for i in range(8021,8031)]\n",
    "cp = [\"{:05d}\".format(i) for i in range(8001,8011)] # 8001 - 8010 , 8011 - 8021, 8021 - 8031\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cp = \"https://www.idealista.com/buscar/alquiler-viviendas/{}/\".format(cp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.idealista.com/buscar/alquiler-viviendas/08001/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(cp)\n",
    "# cp\n",
    "\n",
    "# os.getcwd()\n",
    "\n",
    "parent_path = 'C:\\\\Users\\\\ggari\\\\Desktop\\\\Master_MESIO\\\\TFM\\\\srapper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo un nuevo dataframe para juntar todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(parent_path):\n",
    "    \n",
    "    today = date.today()\n",
    "    directory = \"extraction_{}\".format(today)\n",
    "    \n",
    "    list_files = os.listdir(parent_path)\n",
    "#     path = os.path.join(parent_path, directory)\n",
    "    \n",
    "    if directory not in list_files:\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        os.mkdir(path)\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' created\" % directory)\n",
    "    else:\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' already created\" % directory)\n",
    "    \n",
    "    return directory\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_scrapper(cp,parent_path):\n",
    "    \n",
    "    print(\"parent directory: \",os.getcwd())\n",
    "    \n",
    "    new_dir_name = create_folder(parent_path)\n",
    "    \n",
    "    os.chdir(\".\\\\{}\".format(new_dir_name))\n",
    "\n",
    "    print(\"csv folder: \",os.getcwd())\n",
    "    \n",
    "    for ind, zip_code in enumerate(cp):\n",
    "        print(\"buscando:\",zip_code)\n",
    "        id_from_cp = get_ids(headers, zip_code)\n",
    "        time.sleep(random.randint(2,4)*random.random())\n",
    "        dic_data = obtain_data_from_ids(id_from_cp,headers,cp = zip_code)\n",
    "        time.sleep(random.randint(2,4)*random.random())\n",
    "        df_data = df_parse(dic_to_df(dic_data))\n",
    "\n",
    "        df_data.to_csv(\"data_{}.csv\".format(zip_code),encoding = 'utf-8-sig',index_label= \"id\")\n",
    "#         df_final = df_final.append(df_data)\n",
    "\n",
    "    #     cp.pop(0) da problemas\n",
    "    #     Para saber que cp han sido buscados se agregan a cp_buscados\n",
    "\n",
    "#         cp_buscados.append(zip_code)\n",
    "\n",
    "        print(\"guardado datos de:\",zip_code)\n",
    "        time.sleep(random.randint(2,4)*random.random()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv(parent_path, folder=None):\n",
    "    \n",
    "    colnames = ['id','name','zone','ubicacion_full','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df_final = pd.DataFrame(columns = colnames)\n",
    "    \n",
    "    today = date.today()\n",
    "    \n",
    "    if folder is None:\n",
    "    \n",
    "        directory = \"extraction_{}\".format(today)\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "        except:\n",
    "            print(\"path does not exists\")\n",
    "\n",
    "        csv = os.listdir()\n",
    "    \n",
    "    if folder is not None:\n",
    "        \n",
    "#         today = date.today()\n",
    "#         directory = \"extraction_{}\".format(today)\n",
    "        path = os.path.join(parent_path, folder)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "        except:\n",
    "            print(\"path does not exists\")\n",
    "            \n",
    "        csv = os.listdir()\n",
    "        \n",
    "    for i in csv:\n",
    "        df = pd.read_csv(i)\n",
    "        df_final = pd.concat([df_final,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    df_final.to_csv(\"datos_scrapping_{}.csv\".format(today),encoding = 'utf-8-sig',index=False)\n",
    "    print(\"file '% s' created\" % \"datos_scrapping_{}.csv\".format(today))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent directory:  C:\\Users\\ggari\\Desktop\\1_projects\\TFM\\2_code\\1_extraction\n",
      "Directory 'extraction_2023-02-03' created\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: '.\\\\extraction_2023-02-03'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-19112effd9d2>\u001b[0m in \u001b[0;36mcall_scrapper\u001b[1;34m(cp, parent_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnew_dir_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\\\\{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_dir_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv folder: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: '.\\\\extraction_2023-02-03'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "call_scrapper(cp,parent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists\n",
      "file 'datos_scrapping_2023-01-13.csv' created\n"
     ]
    }
   ],
   "source": [
    "concat_csv(parent_path,folder=\"extraction_2023-01-12\")\n",
    "# parent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
