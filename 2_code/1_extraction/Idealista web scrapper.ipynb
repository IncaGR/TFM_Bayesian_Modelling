{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers -> Ctrl + shift + i (pagina de desarrollador) -> network Ctrl + f5 -> primera pestaña -> request headers\n",
    "# desde \"accept\" ponerlo en dict \n",
    "\n",
    "with open('.\\\\Desktop\\\\1_projects\\\\TFM\\\\2_code\\\\1_extraction\\\\secrets.yaml', 'r') as secrets_file:\n",
    "    secrets = yaml.safe_load(secrets_file)\n",
    "headers = secrets['headers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion usar ids y devolver caracteristicas inmueble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(headers, cp):\n",
    "    '''\n",
    "    Function that returns all the flats ids of certain zip code with fixed headers request.\n",
    "    The function return a list with the ids.\n",
    "    \n",
    "    headers: needed to obtain response 200 (success).\n",
    "    cp: zip code.\n",
    "    \n",
    "    Returns list of ids.\n",
    "    '''\n",
    "    ids = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "\n",
    "        # Get the first page of results\n",
    "        url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/'\n",
    "        r = session.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        soup = bs(r.text, 'lxml')\n",
    "        articles = soup.find_all('article', {'class': 'item'})\n",
    "        ids += [article.get('data-adid') for article in articles]\n",
    "\n",
    "        x=2\n",
    "        \n",
    "    while True:\n",
    "        url = f'https://www.idealista.com/buscar/alquiler-viviendas/{cp}/pagina-{x}.htm'\n",
    "        r = session.get(url, headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            break\n",
    "        soup = bs(r.text, 'lxml')\n",
    "\n",
    "        if soup.find('main', {'class':'listing-items'}).find('div',{'class':'pagination'}) == None:\n",
    "            break\n",
    "\n",
    "\n",
    "        pag_actual = int(soup.find('main', {'class':'listing-items'}).find('div',{'class':'pagination'}).find('li',{'class':'selected'}).text)\n",
    "\n",
    "#         print(x,pag_actual)\n",
    "        if x == pag_actual:\n",
    "                    articles = soup.find('main', {'class':'listing-items'}).find_all('article')\n",
    "        else:\n",
    "            break\n",
    "        articles = soup.find_all('article', {'class': 'item'})\n",
    "        if not articles:\n",
    "            break\n",
    "        ids += [article.get('data-adid') for article in articles]\n",
    "        x += 1\n",
    "        time.sleep(random.randint(3, 4) * random.random())\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data_from_ids(ids, headers, cp):\n",
    "    '''\n",
    "    Function that return all the data from each add id from previous function \"get_ids\".\n",
    "    It returns the data in dictionary format, ready to be converted to dataframe.\n",
    "\n",
    "    ids = list of ids.\n",
    "    headers = to avoid errors, response 200 (success)\n",
    "    cp = zip code of the ids\n",
    "\n",
    "    Returns a dic with id as a key.\n",
    "    '''\n",
    "\n",
    "    dic = {}\n",
    "    url = \"https://www.idealista.com/inmueble/{}/\"\n",
    "    \n",
    "    regex_m2 = re.compile(r'\\d\\d\\d m²|\\d\\d m²')\n",
    "    regex_hab = re.compile(r'\\d hab|sin habitación')\n",
    "    regex_baño = re.compile(r'\\d baño')\n",
    "    regex_planta = re.compile(r'planta \\d\\d|planta \\d|bajo')\n",
    "    \n",
    "    x=1\n",
    "    \n",
    "    for i in ids:\n",
    "        print(\"aqui entra en el bucle\")\n",
    "        with requests.Session() as session:\n",
    "            try:\n",
    "                print(\"aqui esta en el try\")\n",
    "                r = session.get(url.format(i), headers=headers)\n",
    "                r.raise_for_status()\n",
    "\n",
    "                soup = bs(r.text, 'lxml')  # formato mas legible\n",
    "\n",
    "                # ubicacion\n",
    "                ubi = soup.select(\"#headerMap li\")\n",
    "                ubicacion_full = '|'.join([u.get_text(strip=True) for u in ubi])  # string\n",
    "\n",
    "                titulo = soup.select_one('h1').get_text(strip=True)\n",
    "                localizacion = soup.select_one('.main-info__title-block span').get_text(strip=True)\n",
    "\n",
    "                # ubi table\n",
    "                ubicacion_items = [re.sub(r'\\n+', '', u.get_text(strip=True)) for u in ubi]\n",
    "\n",
    "                distrito2 = next((i for i in ubicacion_items if 'Distrito' in i), None)\n",
    "                \n",
    "                barrio2 = next((i for i in ubicacion_items if 'Barrio' in i), None)\n",
    "\n",
    "                calle = ubicacion_items[0]\n",
    "                \n",
    "                barrio = ubicacion_items[1]\n",
    "\n",
    "                distrito = ubicacion_items[2]\n",
    "         \n",
    "                area = ubicacion_items[4]\n",
    "                precio = soup.select_one('.info-data span').get_text(strip=True).strip(' €/mes')\n",
    "\n",
    "                if \".\" in precio:\n",
    "                    precio = precio.replace('.', '')  # falta formato int\n",
    "\n",
    "                precio_down = soup.select_one(\".pricedown_price\")\n",
    "                  \n",
    "                if precio_down != None :\n",
    "                    precio_down = precio_down.text.strip(\"\\n\").strip(\" €\").replace('.','')\n",
    "                else:\n",
    "                    precio_down = precio\n",
    "                \n",
    "#                 detalles basicos del piso habitaciones, baños, etc\n",
    "                detalles = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-one\"})\n",
    "                # info sobre aire acondicionado, psicina, zonas verdes y consumo energetico\n",
    "                detalles2 = soup.find('section',{\"id\":\"details\"}).find(\"div\",{\"class\":\"details-property-feature-two\"})\n",
    "                caract = [det.text.strip() for det in detalles.find_all(\"li\")]\n",
    "\n",
    "                s = ' '.join(caract).lower()\n",
    "\n",
    "                metros = regex_m2.search(s).group().replace(' m²','')\n",
    "\n",
    "                hab =  regex_hab.search(s).group()\n",
    "\n",
    "                wc =  regex_baño.search(s).group()\n",
    "        \n",
    "                terraza = \"\".join(re.findall(r'terraza',s))\n",
    "                balcon = \"\".join(re.findall(r'balcón|balcon',s))\n",
    "                estado = \"\".join(re.findall(r'segunda mano/buen estado',s))\n",
    "                year = \"\".join(re.findall(r'\\d\\d\\d\\d',s))\n",
    "                armarios = \"\".join(re.findall(r'armarios',s))\n",
    "                cocina = \"\".join(re.findall(r'cocina equipada|cocina sin equipar',s))\n",
    "                amu = \"\".join(re.findall(r'amueblada|amueblado|sin amueblar',s))\n",
    "\n",
    "                if regex_planta.search(s) == None:\n",
    "                    planta = 0\n",
    "                else:\n",
    "                    planta = regex_planta.search(s).group()\n",
    "    \n",
    "                calefac = \"\".join(re.findall(r'no dispone de calefacción|calefacción',s))\n",
    "                ascn = \"\".join(re.findall(r'con ascensor|sin ascensor',s))\n",
    "                aire = detalles2.find('ul').text.strip(\"\\n\").lower()\n",
    "                \n",
    "                if aire == \"aire acondicionado\":\n",
    "                    aire = 1\n",
    "                else:\n",
    "                    aire = 0\n",
    "                exterior = \"\".join(re.findall(r'exterior|interior',s))\n",
    "                datalles2 = \"\".join([det.text.replace(\"\\n\",\"|\").lower() for det  in detalles2.find_all('ul')]) # añado zonas verdes, psicina\n",
    "                cp = cp\n",
    "                actualizacion = soup.find('p',{\"class\":\"date-update-text\"}).text\n",
    "  \n",
    "                actualizacion2 = soup.find(\"p\", {\"class\":\"stats-text\"}).text\n",
    "                extract_day = date.today()\n",
    "\n",
    "                dic[i] = [titulo,localizacion,ubicacion_full,distrito2,calle,barrio,barrio2,distrito,area,precio,precio_down,metros,hab,wc,terraza,balcon,estado,year,armarios,cocina,amu,planta,calefac,ascn,aire,exterior,datalles2,cp,actualizacion,actualizacion2,extract_day]\n",
    "                   \n",
    "                len_ids = len(ids)\n",
    "                print(r, \"anuncio número:{}/{} cp: {}\".format(x,len_ids,cp),sep=\"\\n\")\n",
    "                \n",
    "                x += 1\n",
    "                \n",
    "                time.sleep(random.randint(3,4)*random.random())\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"error {e}\")\n",
    "        \n",
    "#                 dic[i] = [\"error\"] * 50\n",
    "                          \n",
    "    return dic\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_to_df(dic):\n",
    "    '''\n",
    "    Transform dic from Idealista https://www.idealista.com/inmueble\n",
    "    to a data frame.\n",
    "    \n",
    "    dic -> Dataframe\n",
    "    '''\n",
    "    colnames = ['name','zone','ubicacion_full','distrito2','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df = pd.DataFrame.from_dict(dic, orient='index',columns=colnames)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_parse(df):\n",
    "    '''\n",
    "    Transform dataframe from Idealista https://www.idealista.com/inmueble\n",
    "    to a clean data frame and convert the data types to int when possible.\n",
    "    \n",
    "    Dataframe -> Dataframe\n",
    "    '''\n",
    "    df['price'] = df.price.astype(int)\n",
    "    df['price_before'] = df.price_before.astype(int)\n",
    "    df['square_mt'] = df.square_mt.astype(int)\n",
    "    try:\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace('sin habitación','0'))\n",
    "        df ['rooms']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df ['wc']=df.wc.apply(lambda x: x.replace(' baño','')).astype(int)\n",
    "    #     df ['wc']=df.rooms.apply(lambda x: x.replace(' hab','')).astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['terraza'] = df.terraza.apply(lambda x:  x.replace('terraza','1') if x == 'terraza' else '0').astype(int)\n",
    "    df['balcon'] = df.balcon.apply(lambda x:  x.replace('balcón','1') if x == 'balcón' else '0').astype(int)\n",
    "    df['armarios'] = df.armarios.apply(lambda x:  x.replace('armarios','1') if x == 'armarios' else '0').astype(int)\n",
    "    df['cocina'] = df.cocina.apply(lambda x:  x.replace('cocina equipada','1') if x == 'cocina equipada' else '0').astype(int)\n",
    "    df['amueblado'] = df.amueblado.apply(lambda x:  x.replace('amueblado','1') if x == 'amueblado' else '0').astype(int)\n",
    "    df['calef'] = df.calef.apply(lambda x:  x.replace('calefacción','1') if x == 'calefacción' else '0').astype(int)\n",
    "    df['asc'] = df.asc.apply(lambda x:  x.replace('con ascensor','1') if x == 'con ascensor' else '0').astype(int)\n",
    "    df['aire'] = df.aire.astype(int)\n",
    "    df['exterior'] = df.exterior.apply(lambda x:  x.replace('exterior','1') if x == 'exterior' else '0').astype(int)\n",
    "    try:\n",
    "        df['planta'] = df.planta.apply(lambda x: x.replace('planta ',''))\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"\" else x)\n",
    "        df['planta'] = df.planta.apply(lambda x: '0' if x == \"bajo\" else x)\n",
    "        df['planta'] = df.planta.astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    df['cp'] = df.cp.astype(str)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista de codigos postales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08001',\n",
       " '08002',\n",
       " '08003',\n",
       " '08004',\n",
       " '08005',\n",
       " '08006',\n",
       " '08007',\n",
       " '08008',\n",
       " '08009',\n",
       " '08010',\n",
       " '08011',\n",
       " '08012',\n",
       " '08013',\n",
       " '08014',\n",
       " '08015',\n",
       " '08016',\n",
       " '08017',\n",
       " '08018',\n",
       " '08019',\n",
       " '08020',\n",
       " '08021',\n",
       " '08022',\n",
       " '08023',\n",
       " '08024',\n",
       " '08025',\n",
       " '08026',\n",
       " '08027',\n",
       " '08028',\n",
       " '08029',\n",
       " '08030',\n",
       " '08031',\n",
       " '08032',\n",
       " '08033',\n",
       " '08034',\n",
       " '08035',\n",
       " '08036',\n",
       " '08037',\n",
       " '08038',\n",
       " '08039',\n",
       " '08040',\n",
       " '08041',\n",
       " '08042']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = [\"{:05d}\".format(i) for i in range(8001,8043)] # 8001 - 8010 , 8011 - 8021, 8021 - 8031\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cp = \"https://www.idealista.com/buscar/alquiler-viviendas/{}/\".format(cp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.idealista.com/buscar/alquiler-viviendas/08017/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent_path = 'C:\\\\Users\\\\ggari\\\\Desktop\\\\1_projects\\\\TFM'\n",
    "\n",
    "data_idealista_path = 'C:\\\\Users\\\\ggari\\\\Desktop\\\\1_projects\\\\TFM\\\\1_data\\\\2_data_Idealista\\\\1_raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiendo un nuevo dataframe para juntar todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(parent_path):\n",
    "    \n",
    "    today = date.today()\n",
    "    directory = \"extraction_{}\".format(today)\n",
    "    \n",
    "    list_files = os.listdir(parent_path)\n",
    "#     path = os.path.join(parent_path, directory)\n",
    "    \n",
    "    if directory not in list_files:\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        os.mkdir(path)\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' created\" % directory)\n",
    "    else:\n",
    "#         os.close(path)\n",
    "        print(\"Directory '% s' already created\" % directory)\n",
    "    \n",
    "    return directory\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_scrapper(cp_list,parent_path):\n",
    "    \n",
    "    print(\"parent directory: \",os.getcwd())\n",
    "    \n",
    "    new_dir_name = create_folder(parent_path)\n",
    "    \n",
    "    os.chdir(\"{}\\\\{}\".format(parent_path,new_dir_name))\n",
    "\n",
    "    print(\"csv folder: \",os.getcwd())\n",
    "    \n",
    "    for chunked_list in chunk(cp_list, 6):\n",
    "        tmp_cp_list = chunked_list\n",
    "        print(tmp_cp_list)\n",
    "    \n",
    "        for ind, zip_code in enumerate(tmp_cp_list):\n",
    "            print(\"buscando:\",zip_code)\n",
    "            id_from_cp = get_ids(headers, zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "            dic_data = obtain_data_from_ids(id_from_cp,headers,cp = zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random())\n",
    "            df_data = df_parse(dic_to_df(dic_data))\n",
    "\n",
    "            df_data.to_csv(\"data_{}.csv\".format(zip_code),encoding = 'utf-8-sig',index_label= \"id\")\n",
    "    #         df_final = df_final.append(df_data)\n",
    "\n",
    "        #     cp.pop(0) da problemas\n",
    "        #     Para saber que cp han sido buscados se agregan a cp_buscados\n",
    "\n",
    "    #         cp_buscados.append(zip_code)\n",
    "\n",
    "            print(\"guardado datos de:\",zip_code)\n",
    "            time.sleep(random.randint(2,4)*random.random()) \n",
    "\n",
    "        time.sleep(random.randint(750,900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv(parent_path, folder=None):\n",
    "    \n",
    "    colnames = ['id','name','zone','ubicacion_full','calle','barrio','barrio2','distrito','area','price','price_before','square_mt','rooms','wc','terraza','balcon','estado',\\\n",
    "     'año','armarios','cocina','amueblado','planta','calef','asc','aire','exterior','datalles2','cp','actualizacion','actualizacion2','extract_day']\n",
    "    df_final = pd.DataFrame(columns = colnames)\n",
    "    \n",
    "#     today = date.today()\n",
    "    \n",
    "    if folder is None:\n",
    "        today = date.today()\n",
    "        directory = \"extraction_{}\".format(today)\n",
    "        path = os.path.join(parent_path, directory)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "            csv = os.listdir()\n",
    "        except ValueError:\n",
    "            print(\"path does not exists\")\n",
    "\n",
    "        \n",
    "    \n",
    "    if folder is not None:\n",
    "\n",
    "        path = os.path.join(parent_path, folder)\n",
    "        try:\n",
    "            os.chdir(path)\n",
    "            print(\"path exists\")\n",
    "            csv = os.listdir()\n",
    "        except ValueError:\n",
    "            print(\"path does not exists\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    for i in csv:\n",
    "        df = pd.read_csv(i)\n",
    "        df_final = pd.concat([df_final,df],axis=0,ignore_index=True)\n",
    "        \n",
    "    df_final.to_csv(\"datos_scrapping_{}.csv\".format(today),encoding = 'utf-8-sig',index=False)\n",
    "    print(\"file '% s' created\" % \"datos_scrapping_{}.csv\".format(today))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # call_scrapper(cp,data_idealista_path)\n",
    "# call_scrapper(cp,data_idealista_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists\n",
      "file 'datos_scrapping_2023-05-20.csv' created\n"
     ]
    }
   ],
   "source": [
    "concat_csv(data_idealista_path,folder=\"extraction_2023-05-30\")\n",
    "# parent_path\n",
    "# data_idealista_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"extraction_2023-04-11\"\n",
    "# re.search('\\d{4}-\\d{2}-\\d{2}', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
